# -*- coding: utf-8 -*-
"""AUTO_INSURANCE_PROJECT (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lxz2hbUX3Ztw1Xz34ryfl00GK5tot0NU

### **Aim of the Project**
**The aim of the project is to build a Machine Learning
Model to predict whether an owner will initiate an auto
insurance claim in the next year.**

The auto insurance industry is witnessing a paradigm shift. Since auto insurance company consists of homogenous good thereby making it difficult to differentiate product A from product B, also companies are fighting a price war (for insurance price). On top of that, the
distribution channel is shifting more from traditional insurance brokers to online purchases,which means that the ability for companies to interact through human touchpoints is limited,
and customers should be quoted at a reasonable price. A good price quote is one that makes the customer purchase the policy and helps the company to increase the profits. Also, the insurance premium is calculated based on more than 50+ parameters, which means that traditional business analytics-based algorithms are now limited in their ability to differentiate among customers based on subtle parameters.
"""

from google.colab import drive
drive.mount('/content/drive',force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler,Normalizer
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score,balanced_accuracy_score,accuracy_score,classification_report,confusion_matrix
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,train_test_split
from sklearn.svm import LinearSVC
from sklearn.model_selection import cross_val_score
import xgboost
from scipy import stats

data=pd.read_csv('/content/drive/My Drive/auto_insurance_data.csv')
data=data.drop(['id'],axis=1)
data.shape

data.describe()

"""From the initial glance of the dataset, it can be observed that the minimum values in some features are -1, indicating the presence of missing values and some features have negative mean,indicating large number of missing values."""

#data.info()

# Splitting the columns on the basis of feature type
cat_var=[]
bin_var=[]
intr_var=[]
ord_var=[]
target=[]
for i in data.columns:
  if 'cat' in i:
    cat_var.append(i)
  elif 'bin' in i:
    bin_var.append(i)
  elif data[i].dtype==float:
    intr_var.append(i)
  elif (data[i].dtype==int and i!='target'):
    ord_var.append(i)
  elif i == 'target':
    target.append(i)
  else:
    othr_var.append(i)

"""### **INTERVAL DATA**"""

# Separating the data with target =1
target_1=data[data['target']==1]

plt.subplots(1,4,figsize=(20,3))
plt.subplot(1,4,1)
sns.distplot(data[intr_var[2]],kde=True,bins=40,color='r',kde_kws={"color": "k", "lw": 3})
plt.subplot(1,4,2)
sns.distplot(data[intr_var[3]],kde=True,bins=30,color='g',kde_kws={"color": "k", "lw": 2})
plt.subplot(1,4,3)
sns.distplot(data[intr_var[4]],kde=True,bins=50,color='r',kde_kws={"color": "k", "lw": 3})
plt.subplot(1,4,4)
sns.distplot(data[intr_var[5]],kde=True,bins=30,color='orange',kde_kws={"color": "k", "lw": 3})
plt.show()
plt.subplots(1,4,figsize=(20,3))
plt.subplot(1,4,1)
sns.distplot(target_1[intr_var[2]],kde=True,bins=45,color='b',kde_kws={"color": "k", "lw": 3})
plt.ylabel('Claimed Percentage %')
plt.subplot(1,4,2)
sns.distplot(target_1[intr_var[3]],kde=True,bins=55,color='g',kde_kws={"color": "k", "lw": 2})
plt.subplot(1,4,3)
sns.distplot(target_1[intr_var[4]],kde=True,bins=50,color='r',kde_kws={"color": "k", "lw": 3})
plt.subplot(1,4,4)
sns.distplot(target_1[intr_var[5]],kde=True,bins=40,color='orange',kde_kws={"color": "k", "lw": 3})
plt.show()

def intr_plot(i,width):
   plt.title(intr_var[i])
   y=round(data[intr_var[i]].groupby(data[intr_var[i]]).count()*100/len(data['target']),2)
   plt.bar(y.index,y.values,width=width,color='r',label='Original Distribution')
   plt.xticks(np.linspace(data[intr_var[i]].min(),data[intr_var[i]].max(),5))
   y=round(target_1[intr_var[i]].groupby(target_1[intr_var[i]]).count()*100/len(target_1['target']),2)
   plt.bar(y.index,y.values,width=width,color='b',label='Claimed Distribution')
   plt.xticks(np.linspace(target_1[intr_var[i]].min(),target_1[intr_var[i]].max(),5))

plt.subplots(1,6,figsize=(20,4))
plt.subplot(1,6,1)
intr_plot(0,0.1)
plt.ylabel('Percentage %')
plt.legend()
plt.subplot(1,6,2)
intr_plot(1,0.07)
plt.subplot(1,6,3)
intr_plot(6,0.1)
plt.subplot(1,6,4)
intr_plot(7,0.06)
plt.subplot(1,6,5)
intr_plot(8,0.05)
plt.subplot(1,6,6)
intr_plot(9,0.05)

"""- The **reg** and **car** features are showing both and positively and negatively skewed distribution. The **ps_reg_01** and **ps_car_15** features are showing an unusual spike at higher values.
- The **calc** features i.e. calc_01,2 & 3 are showing uniform distribution.
- The Feature **ps_reg_02** shows positive distribution with maximum frequency occuring at 0.2 whereas **ps_car_15** shows negative distribution with maximum frequency occuring at 3.6.

### **CATEGORICAL DATA**

#### **Distribution of features**
"""

plt.style.use('ggplot')
plt.subplots(3,4,figsize=(20,10))
plt.subplots_adjust(bottom=-0.1)
for i in range(len(cat_var[:-2])):
  plt.subplot(3,4,i+1)
  plt.title(cat_var[i])
  data[cat_var[i]].hist(align='left',color='b',label='Overall')
  target_1[cat_var[i]].hist(align='left',label='Claimed')
  plt.xticks(data[cat_var[i]].unique())
  plt.legend()
plt.subplots(1,2,figsize=(10,4))
plt.subplot(1,2,1)
data[cat_var[-2]].hist(align='left',color='b')
target_1[cat_var[-2]].hist(align='left')
plt.xticks(data[cat_var[-2]].unique())
plt.title(cat_var[-2])
plt.subplot(1,2,2)
data[cat_var[-1]].hist(align='left',color='b')
target_1[cat_var[-1]].hist(align='left')
plt.xticks(data[cat_var[-1]].unique())
plt.title(cat_var[-1])
plt.show()

"""- It can be observed that the categorical features have varying labels.
- Some of the features such as **ps_ind_05_cat,ps_car_02_cat,ps_car_10_cat,ps_car_07_cat,ps_car_08_cat** are very unbalanced.
- The feature **ps_car_11_cat** have 104 unique labels and maximum frequency occuring at 104.
- The feature **ps_car_11_cat** looks similar to interval features such as **ps_reg_01** and **ps_car_15** in distribution.
- In feature **ps_car_03_cat**, the percentage of -1 i.e. missing values is very high and can't be treated as a label.

**Exploring the contribution of each feature towards insurance claim.**
"""

new_cat_var=target_1[cat_var].nunique().sort_values().index

# Plotting function
def cat_bar_plot(i,width):
  plt.title(new_cat_var[i])
  y=target_1[new_cat_var[i]].groupby(target_1[new_cat_var[i]]).count()*100/len(target_1['target'])
  plt.bar(y.index,y.values,width=width,color=['black', 'red', 'green', 'blue', 'cyan'])
  plt.xticks(target_1[new_cat_var[i]].unique())
  plt.ylabel("Percentage claimed %")
  #plt.xlabel("Classes")

plt.subplots(2,3,figsize=(17,7),sharex=True,sharey=True)
plt.subplot(2,3,1)
cat_bar_plot(0,0.1)
plt.subplot(2,3,2)
cat_bar_plot(1,0.1)
plt.subplot(2,3,3)
cat_bar_plot(2,0.2)
plt.subplot(2,3,4)
cat_bar_plot(3,0.2)
plt.subplot(2,3,5)
cat_bar_plot(4,0.2)
plt.subplot(2,3,6)
cat_bar_plot(5,0.2)
plt.show()

"""- The correlation of class 1 of the feature **ps_car_07_cat** with claimed percentage is very significant followed by **ps_car_02_cat & ps_car_08_cat**."""

plt.subplots(1,2,figsize=(17,4),sharex=True,sharey=True)
plt.subplot(1,2,1)
cat_bar_plot(6,0.2)
plt.subplot(1,2,2)
cat_bar_plot(7,0.4)
plt.show()

"""- It appears that the correlation of class1 of the**ps_car_10_cat** is approximately 99% but from the distribution it can be seen that this feature is highly imbalanced, with frequency of class 0 as almost negligible. So this feature will not help much in prediction.

- Interestingly, the correlation of **ps_ind_02_cat** with claimed percentage and the distribution of this feature appears similar.
"""

plt.subplots(1,2,figsize=(17,4),sharex=True,sharey=True)
plt.subplot(1,2,1)
cat_bar_plot(8,0.3)
plt.subplot(1,2,2)
cat_bar_plot(9,0.4)
plt.show()

"""- In **ps_ind_05_cat** the class 0 is highly dominating that is why this class appears to be highly correlated with claimed percentage.
- Class 0 and 2 in **ps_car_09_cat** shows significant correlation with claimed percentage.
"""

plt.subplots(1,3,figsize=(20,4),sharex=True,sharey=True)
plt.subplot(1,3,1)
cat_bar_plot(10,0.6)
plt.subplot(1,3,2)
cat_bar_plot(11,0.6)
plt.subplot(1,3,3)
cat_bar_plot(12,0.6)
plt.show()

"""- All three features have correlation similar to their distribution."""

plt.figure(figsize=(15,4))
plt.title(new_cat_var[13])
y=target_1[new_cat_var[13]].groupby(target_1[new_cat_var[13]]).count()*100/len(target_1['target'])
plt.bar(y.index,y.values,color=['black', 'red', 'green', 'blue', 'cyan'])
plt.ylabel("Percentage claimed %")
plt.xlabel("Classes")
plt.show()
x=round((target_1[new_cat_var[13]].groupby(target_1[new_cat_var[13]]).count()*100/len(target_1['target'])).sort_values(ascending=False),2)
print('Largest contribution by label=',x.index[0],' Percentage=',x.values[0],'%')

"""There are many labels in this feature with almost similar claim percentage except for label 104 that has a percentage of 17.5%.

### **BINARY DATA**

There are total 17 binary features in the dataset. Let's check the distribution of the features
"""

plt.subplots(4,4,figsize=(20,10))
plt.subplots_adjust(bottom=-0.1)
plt.style.use('ggplot')
for i in range(len(bin_var[:-1])):
  plt.subplot(4,4,i+1)
  plt.title(bin_var[i])
  data[bin_var[i]].hist(align='left')
  plt.xticks(data[bin_var[i]].unique())
plt.subplots(1,1,figsize=(5,3))
plt.subplot(1,1,1)
data[bin_var[-1]].hist(align='left')
plt.xticks(data[bin_var[-1]].unique())
plt.title(bin_var[-1])
plt.show()

"""- The above plots show the distribution of binary features.
- In features **ps_ind_10,11,12 & 13** the frequency of class 1 is almost negligible to none. Therefore they will not be able to contribute towards prediction.
- There appears to be a significant difference in frequency in some of the features.

**Analysing the contribution of individual binary feature towards insurance claim.**
"""

plt.subplots(4,4,figsize=(20,10))
plt.subplots_adjust(bottom=-0.1)
for i in range(len(bin_var[:-1])):
  plt.subplot(4,4,i+1)
  plt.title(bin_var[i])
  y=round(target_1[bin_var[i]].groupby(target_1[bin_var[i]]).count()*100/len(target_1['target']),2)
  plt.bar(y.index,y.values,width=0.1,color=['blue', 'cyan','black', 'red', 'green'])
  plt.xticks(data[bin_var[i]].unique())
  plt.ylabel('Claim Percentage %')
plt.figure(figsize=(5,3))
x=round(target_1[bin_var[-1]].groupby(target_1[bin_var[-1]]).count()*100/len(target_1['target']),2)
plt.bar(x.index,x.values,width=0.1,color=['blue', 'cyan'])
plt.xticks(data[bin_var[i]].unique())
plt.ylabel('Claim Percentage %')
plt.title(bin_var[-1])
plt.show()

"""- In most of the features, the class 0 is commanding maximum claim percentage except ps_ind_16_bin,ps_calc_16_bin and ps_calc_17_bin.
- Claim percentages of Class 0 of ps_ind_10,11,12 and 13 features are almost 99% but it's not a matter of surprise as the percentages of class one all these features are almost negligible.

### **ORDINAL DATA**

**Visualizing the feature distribution**
"""

plt.subplots(4,4,figsize=(20,12))
plt.subplots_adjust(bottom=-0.1)
plt.style.use('ggplot')
for i in range(len(ord_var)):
  plt.subplot(4,4,i+1)
  plt.title(ord_var[i])
  data[ord_var[i]].hist(align='left',color='blue')
  plt.xticks(data[ord_var[i]].unique())
plt.show()

"""- It can be observed that in feature **ps_ind_14** is very imbalanced the percentage of one is almost negligible, hence this feature will not have any impact on the prediction. 
- The features such as **ps_car_11,ps_calc_04,5,8,12** showing skewed distribution.    
- The features **ps_calc_07,10,11 and 14** showing approximately normal distribution.

**Checking the impact of individual ordinal feature towards target feature**
"""

# Defining function to plot bar graph
def ord_plot(i,width):
   plt.title(ord_var[i])
   y=round(target_1[ord_var[i]].groupby(target_1[ord_var[i]]).count()*100/len(target_1['target']),2)
   plt.bar(y.index,y.values,width=width,color=['blue', 'cyan','black', 'red', 'green'])
   plt.xticks(data[ord_var[i]].unique())
   plt.ylabel('Claim Percentage %')

plt.subplots(4,4,figsize=(20,10))
plt.subplots_adjust(bottom=-0.3)
plt.subplot(4,4,1)
ord_plot(0,0.4)
plt.subplot(4,4,2)
ord_plot(1,0.5)
plt.subplot(4,4,3)
ord_plot(2,0.3)
plt.subplot(4,4,4)
ord_plot(3,0.5)
plt.subplot(4,4,5)
ord_plot(4,0.3)
plt.subplot(4,4,6)
ord_plot(5,0.3)
plt.subplot(4,4,7)
ord_plot(6,0.3)
plt.subplot(4,4,8)
ord_plot(7,0.3)
plt.subplot(4,4,9)
ord_plot(8,0.4)
plt.subplot(4,4,10)
ord_plot(9,0.4)
plt.subplot(4,4,11)
ord_plot(10,0.4)
plt.subplot(4,4,12)
ord_plot(11,0.7)
plt.subplot(4,4,13)
ord_plot(12,0.6)
plt.subplot(4,4,14)
ord_plot(13,0.4)
plt.subplot(4,4,15)
ord_plot(14,0.5)
plt.subplot(4,4,16)
ord_plot(15,0.6)

"""On visualizing the impact of individual feature towards claim percentage, it can be observed that:- <br/>
 - **pd_ind_14** doesn't have much impact as proportion of one and zero is highly imbalanced.
 - **ps_ind_03** and **ps_ind_05** , many feature elements have similar impact with 3 and 7 having maximum share repectively.
 - **ps_ind_01,ps_car_11** and **ps_calc_12** are having positively and negatively skewed distribution with 0,3 and 1 are having maximum percentage respectively.
 - **ps_calc_04,5,6,7,8,9,10,11,13 and 14**, features showing normal distribution with varied standard deviation.

### **MISSING VALUES**
"""

# CREATING A LIST OF FEATURES
features=list(data.columns)

"""**Computing the percentage of missing values in the original dataset.**"""

# CHECK FOR MISSING VALUES IN THE DATA. MISSING VALUES ARE REPRESENTED AS -1
print('FEATURE','                        COUNT','          MISSING VALUES (%)')
print('')
missing_val_feat=[]
per1=[]
for i in features:
  count=0
  mylist=list(data[i])
  count=mylist.count(-1)
  per=round(count*100/data[i].count(),2)
  if count!=0:
    missing_val_feat.append(i)
    per1.append(per)
    print(f'{i:{20}}{count:{15}}{per:{25}}')

"""- It can be observed that the percentages of missing values are high in **ps_reg_03,ps_car_03_cat and ps_car_05_cat**. Imputing the missing values in these features might not give reliable result so its better to remove these features.
- The features having missing values 1% or less can be removed row wise without significant data loss.But first let's check how much these missing values sharing claim percentage.

**Calculating the percentage of missing values when the target feature containing only ones i.e for insurance claimed.**
"""

#print('FEATURE','                        COUNT','          MISSING VALUES (%)')
print('')
missing_val_feat1=[]
per2=[]
for i in features:
  count=0
  mylist=list(target_1[i])
  count=mylist.count(-1)
  per=round(count*100/target_1[i].count(),2)
  if count!=0:
    missing_val_feat1.append(i)
    per2.append(per)
    #print(f'{i:{20}}{count:{15}}{per:{25}}')
pd.DataFrame([per1,per2],index=['Overall %','Within Claimed %'],columns=missing_val_feat).T

"""- On comparing the missing values percentage in overall feature and within claimed, it can be seen that percentage has increased. 
- Removing these missing values can lead to loss of important claimed data.Hence its better to impute these values with central tendencies.
"""

#Replacing -1 with None
data1=data.replace({-1:np.nan})
#data1.head(10)

"""**Visualizing the missing values**"""

data1.isnull().mean().plot.bar(figsize=(25,4))
plt.show()

plt.figure(figsize=(25,5))
sns.heatmap(data1.isnull(),yticklabels=False,cmap='plasma')
plt.show()

"""**Strategy for Missing values**

- The percentage of missing values in the features **"ps_car_03_cat"** and **"ps_car_05_cat"** is very large.Hence dropping these feature columns.

- **ps_reg_03** and **ps_car_14** have 18.11 % and 7.16 % of missing values respectively, so imputing missing values on the basis of distribution. 

- For the remaining features, impute missing values by mean,median or mode.
"""

#Dropping "ps_car_03_cat" and "ps_car_05_cat"
data1=data1.drop(["ps_car_03_cat","ps_car_05_cat"],axis=1)
data1.shape

"""**Categorical features**
- From the visualizations of categorical features, it can be observe that the missing values in the features **ps_ind_05_cat and ps_car_07_cat** can be safely imputed with their respective modes.
- The missing values in the remaining features will be treated as a separate class in order to avoid any bias in the dataset.
"""

data1['ps_ind_05_cat'].fillna(int(data1['ps_ind_05_cat'].mode()),inplace=True)
data1['ps_car_07_cat'].fillna(int(data1['ps_car_07_cat'].mode()),inplace=True)

data1['ps_ind_02_cat'].fillna(-1,inplace=True)
data1['ps_ind_04_cat'].fillna(-1,inplace=True)
data1['ps_car_01_cat'].fillna(-1,inplace=True)
data1['ps_car_09_cat'].fillna(-1,inplace=True)

"""**Interval Features**
- For ps_reg_03 first, we will analyse the distribution of the features before and after imputation.
"""

# Distribution of features before imputation.
sns.distplot(data1['ps_reg_03'])
plt.show()

reg_03_mean=data1['ps_reg_03'].fillna(data1['ps_reg_03'].mean())
reg_03_mode=data1['ps_reg_03'].fillna(0.633936)
reg_03_median=data1['ps_reg_03'].fillna(data1['ps_reg_03'].median())

"""**Selecting the best strategy for missing values imputation.**"""

plt.figure(figsize=(10,6))
sns.distplot(reg_03_mean,color='green',kde_kws={"color": "g", "lw": 3,"label":"Mean"})
sns.distplot(reg_03_mode,color='orange',kde_kws={"color": "orange", "lw": 3,"label":"Mode"})
sns.distplot(reg_03_median,color='r',kde_kws={"color": "r", "lw": 3,"label":"Median"})
sns.distplot(data1['ps_reg_03'],color='blue',kde_kws={"color": "k", "lw": 3,"label":"Original"})
plt.show()

"""- By comparative analysis of different strategies with respect to the  original feature, it can be seen that the apex of the distribution has changed significantly in the vertical direction.
- In order to preserve the distribution, let's try fillna() method with ffill or bfill strategy.
"""

reg_03_ffill=data1['ps_reg_03'].fillna(method='ffill')
sns.distplot(reg_03_ffill,color='r',kde_kws={"color": "r", "lw": 3,"label":"ffill"})
sns.distplot(data1['ps_reg_03'],color='blue',kde_kws={"color": "k", "lw": 3,"label":"Original"})
plt.show()

"""- By using fillna() method, the distribution is completely preserved.

#### **2-Sample Kolmogorov–Smirnov test to check the similarity of distributions**
"""

# Dropping null values for KS-test
ps_reg_03_dropped=data1['ps_reg_03'].dropna()
from scipy import stats
print(stats.ks_2samp(reg_03_mean,ps_reg_03_dropped,alternative='two-sided',mode='exact'))
print(stats.ks_2samp(reg_03_median,ps_reg_03_dropped,alternative='two-sided',mode='exact'))
print(stats.ks_2samp(reg_03_mode,ps_reg_03_dropped,alternative='two-sided',mode='exact'))
print(stats.ks_2samp(reg_03_ffill,ps_reg_03_dropped,alternative='two-sided',mode='exact'))

"""Null Hypothesis.......**H0:** Distributions are same <br/>
Alternate Hypothesis **H1:** Distributions are different
- Surprisingly, from the KS-test the p-values of **mean,median and mode & ffill** comes out to be exact 1.0 which means that the distribution of the feature before and after imputation of missing values is same. Hence we accept the null hypothesis.

- However, in order to preserve the shape of the distribution the Fillna method with **ffill** strategy will be used.Therefore,imputing missing values with ffill (forward fill).
"""

data1['ps_reg_03'].fillna(method='ffill',inplace=True)

"""#### **Missing value imputation of ps_car_14 feature.**"""

sns.distplot(data1['ps_car_14'],kde_kws={"color": "k", "lw": 3,"label":"Original"})
plt.show()

car_14_mean=data1['ps_car_14'].fillna(data1['ps_car_14'].mean())
car_14_median=data1['ps_car_14'].fillna(data1['ps_car_14'].median())
car_14_mode=data1['ps_car_14'].fillna(0.361525)
car_14_ffill=data1['ps_car_14'].fillna(method='ffill')

plt.subplots(1,4,figsize=(20,4))
plt.subplot(1,4,1)
sns.distplot(car_14_mean,color='green',kde_kws={"color": "green", "lw": 3,"label":"Mean"})
sns.distplot(data1['ps_car_14'],color='blue',kde_kws={"color": "k", "lw": 3,"label":"Original"})
plt.subplot(1,4,2)
sns.distplot(car_14_median,color='orange',kde_kws={"color": "orange", "lw": 3,"label":"Median"})
sns.distplot(data1['ps_car_14'],color='blue',kde_kws={"color": "k", "lw": 3,"label":"Original"})
plt.subplot(1,4,3)
sns.distplot(car_14_mode,color='red',kde_kws={"color": "red", "lw": 3,"label":"Mode"})
sns.distplot(data1['ps_car_14'],color='blue',kde_kws={"color": "k", "lw": 3,"label":"Original"})
plt.subplot(1,4,4)
sns.distplot(car_14_ffill,color='cyan',kde_kws={"color": "cyan", "lw": 3,"label":"ffill"})
sns.distplot(data1['ps_car_14'],color='blue',kde_kws={"color": "k", "lw": 3,"label":"Original"})
plt.show()

"""#### **2-Sample Kolmogorov–Smirnov test to check the similarity of distributions**"""

ps_car_14_dropped=data1['ps_car_14'].dropna()
print(stats.ks_2samp(car_14_mean,ps_car_14_dropped,mode='asymp'))
print(stats.ks_2samp(car_14_median,ps_car_14_dropped,mode='asymp'))
print(stats.ks_2samp(car_14_mode,ps_car_14_dropped,mode='asymp'))
print(stats.ks_2samp(car_14_ffill,ps_car_14_dropped,mode='asymp'))

"""From the visualizations of the distributions and KS-test, it can be seen that filling the missing values with forward fill method in fillna(), best preserved the distribution of the feature with p-value as 1.0"""

# Imputing the missing values
data1['ps_car_14'].fillna(method='ffill',inplace=True)

# Imputing missing values of the remaining features with mode and mean respectively.
data1['ps_car_11'].fillna(int(data1['ps_car_11'].mode()),inplace=True)
data1['ps_car_12'].fillna(data1['ps_car_12'].mean(),inplace=True)

# Final check for missing values
data1.isna().sum().sum()

# Creating a copy of the dataset
data2=data1.copy()

"""#### **Restoring the original dtypes of the features.**"""

# Separating the data types into two categories
int_feat=[]
float_feat=[]
for i in data1.columns:
  if data[i].dtypes=='int64':
    int_feat.append(i)
  else:
    float_feat.append(i)

for i in int_feat:
  data2[i]=data2[i].astype('int64')
for i in float_feat:
  data2[i]=data2[i].astype('float64')

#data2.info()

# Splitting the columns
categorical=[]
binary=[]
interval=[]
ordinal=[]
target=[]
for i in data2.columns:
  if 'cat' in i:
    categorical.append(i)
  elif 'bin' in i:
    binary.append(i)
  elif data2[i].dtype==float:
    interval.append(i)
  elif (data2[i].dtype==int and i!='target'):
    ordinal.append(i)
  elif i == 'target':
    target.append(i)

"""### **One Hot Encoding of Categorical Variables**"""

dummy_df=pd.DataFrame()
for i in range(len(categorical)):
  dummy=pd.get_dummies(data2[categorical[i]],prefix=categorical[i],drop_first=True,dtype='int64')
  for i in range(len(dummy.columns)):
    dummy_df[dummy.columns[i]]=dummy.iloc[:,i]

dummy_df.head(2)

"""### **Scaling of Interval and Ordinal features**"""

scaler=MinMaxScaler()
interval_scaled=scaler.fit_transform(data2[interval])
interval_scaled=pd.DataFrame(interval_scaled,columns=interval)
interval_scaled.head(2)

ordinal_scaled=scaler.fit_transform(data2[ordinal])
ordinal_scaled=pd.DataFrame(ordinal_scaled,columns=ordinal)
ordinal_scaled.head(2)

"""### **Merging all the features**"""

data3=pd.concat([data2['target'],interval_scaled,ordinal_scaled,dummy_df,data2[binary]],axis=1)
data3.head(2)

"""After One-Hot encoding, the total number of columns have rose to 207.

### **BALANCING DATA**
"""

# Checking the proportion of the target
x=round(data['target'].groupby(data['target']).count()*100/len(data['target']),2)
plt.bar(data['target'].unique(),x,width=0.1,color='blue')
plt.xticks(data['target'].unique(),['False','True'])
plt.xlabel("Claims")
plt.ylabel("Percentage (%)")
plt.title("Insurance Claimed")
plt.show()
print('%')
print(x)

"""- It can be seen that the dataset is highly imbalanced. The percentage of insurance claimed is just **3.64 %** of overall claims.
- In order to balance the data we need to perform **oversampling**.
"""

X=data3.drop(['target'],axis=1)
Y=data3['target']

from imblearn.over_sampling import RandomOverSampler
ros=RandomOverSampler(ratio=1)
X_train_res,y_train_res=ros.fit_sample(X,Y)

# Oversampled data with one-hot encoded
X_train_res.shape,y_train_res.shape

X_train_res=pd.DataFrame(X_train_res,columns=X.columns)
y_train_res=pd.DataFrame(y_train_res,columns=['target'])

from collections import Counter
print('Original dataset shape: {}'.format(Counter(Y)))
print('Resampled dataset shape: {}'.format(Counter(y_train_res)))

# Oversampling the data without one-hot encoding and scaling
data2_X = data2.drop('target',axis=1)
data2_Y=data2['target']
ros1=RandomOverSampler(ratio=1)
X_train3_res,Y_train3_res=ros.fit_sample(data2_X,data2_Y)
X_train3_res=pd.DataFrame(X_train3_res,columns=data2_X.columns)
Y_train3_res=pd.DataFrame(Y_train3_res,columns=['target'])
X_train3_res.shape,Y_train3_res.shape

"""### **CORRELATION PLOTS**"""

# Interval
corr = data2[['target','ps_reg_03','ps_car_12','ps_car_13','ps_car_14','ps_reg_01','ps_reg_02']].corr()
corr.style.background_gradient(cmap='coolwarm').set_precision(2)

"""- There is a good correlation between **ps_reg_03** and **ps_reg_02**.
- The **ps_car_12** showing moderate correlation with **ps_reg_02 and 03** and strong correlation with **ps_car_12 and 14**.
"""

# ORDINAL
corr1=data2[['target','ps_car_11','ps_ind_01','ps_ind_03','ps_ind_14','ps_ind_15']].corr()
corr1.style.background_gradient(cmap='coolwarm').set_precision(2)

"""**ps_ind_03** showing moderate correlation with **ps_ind_01**."""

# BINARY
corr3=data2[['target','ps_ind_06_bin','ps_ind_07_bin','ps_ind_08_bin','ps_ind_09_bin',
             'ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin','ps_ind_13_bin','ps_ind_16_bin','ps_ind_17_bin','ps_ind_18_bin']].corr()
corr3.style.background_gradient(cmap='coolwarm').set_precision(2)

"""- In binary features **ps_ind_07_bin** showing negative correlation with **ps_ind_06_bin**.
- **ps_ind_08_bin and 09_bin** showing negative correlation with **ps_ind_06_bin,07_bin & 08_bin**.
- **ps_ind_18_bin and ps_ind_17_bin** showing maximum correlation with **ps_ind_16_bin**.
"""

# CATEGORICAL
corr4=data2[['target','ps_ind_02_cat','ps_ind_04_cat','ps_ind_05_cat','ps_car_01_cat','ps_car_02_cat','ps_car_04_cat',
 'ps_car_06_cat','ps_car_07_cat','ps_car_08_cat','ps_car_09_cat','ps_car_10_cat','ps_car_11_cat']].corr()
corr4.style.background_gradient(cmap='coolwarm').set_precision(2)

"""- The categorical features doesn't have much coorelation among one another except between **ps_car_02_cat and, ps_car_04_cat and ps_car_06_cat**.

### **Significance Test for the Correlation**
- Null Hypothesis.......**H0:** No correlation
- Alternate Hypothesis **H1:** Correlation exists
"""

# Defining function for hypothesis testing
def corr_test(feat1,feat2,alpha):
  df=len(data2[feat1])-2
  alpha=1-(alpha/2)
  t1=stats.t.ppf(alpha,df)
  r=stats.pearsonr(data2[feat1],data2[feat2])[0]
  n=len(data2[feat1])
  t2=r/np.sqrt((1-(r*r))/df)
  print(feat1,feat2)
  if t1>t2 or t1<-t2:
    print('Correlation Exists ! Reject the Null Hypothesis')
  else:
    print('No Correlation ! Accept the Null Hypothesis')
  print('')

corr_test('ps_reg_03','ps_reg_02',0.05)
corr_test('ps_car_12','ps_reg_02',0.05)
corr_test('ps_car_12','ps_reg_03',0.05)
corr_test('ps_car_12','ps_car_12',0.05)
corr_test('ps_car_12','ps_car_14',0.05)
corr_test('ps_ind_03','ps_ind_01',0.05)
corr_test('ps_ind_07_bin','ps_ind_06_bin',0.05)
corr_test('ps_ind_08_bin','ps_ind_06_bin',0.05)
corr_test('ps_ind_09_bin','ps_ind_07_bin',0.05)
corr_test('ps_ind_18_bin','ps_ind_16_bin',0.05)
corr_test('ps_ind_17_bin','ps_ind_16_bin',0.05)
corr_test('ps_car_02_cat','ps_car_04_cat',0.05)
corr_test('ps_car_02_cat','ps_car_06_cat',0.05)
corr_test('ps_car_04_cat','ps_car_06_cat',0.05)

"""**From the above result, it is verified whether correlation between two features exists or not!**

### **EDA SUMMARY**

- In EDA, we analyzed the distribution of each feature category and its impact on insurance claims. In interval features, we observed that the distribution and claimed percentage distribution is almost same except for ps_reg_01 and ps_car_15 that show higher claim percentage.
Some of the categorical and binary features show very high claim percentage but on the other hand those features were found out to be imbalanced.
- In most of the features, the claim rates are almost proportional to their original distribution with small difference in terms of percentages.The **ind** features have more impact on claims whereas the calc binary and interval features have very low impact and hence will not be able to help in prediction.

### **FEATURE SELECTION**

#### **Univariate Selection**
"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

best_features=SelectKBest(k=50,score_func=chi2)
fit=best_features.fit(X,Y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['X','Score']
featureScores=featureScores.sort_values(by='Score',ascending=False)
featureScores.head()

selected_features=X_train_res[featureScores['X'][:]]
selected_features.head(2)

"""#### **Feature Importance**"""

from sklearn.ensemble import ExtraTreesClassifier
etc = ExtraTreesClassifier()
etc.fit(X,Y)

plt.figure(figsize=(15,15))
feat_importances = pd.Series(etc.feature_importances_, index=X.columns)
feat_importances.nlargest(50).plot(kind='barh')
plt.show()

imp_features=pd.DataFrame(etc.feature_importances_)
imp_features_cols=pd.DataFrame(etc.feature_importances_,index=X.columns,columns=['importance'])
imp_features_cols=imp_features_cols['importance'].sort_values(ascending=False)
imp_features=imp_features_cols.index.to_list()

"""## **MODELLING**"""

def metrics(test,pred):
  print('ACCURACY=',accuracy_score(test,pred))
  print('BALANCED ACCURACY=',balanced_accuracy_score(test,pred))
  print('F1_SCORE=',f1_score(test,pred))
  print('CONFUSION MATRIX')
  print(confusion_matrix(test,pred))
  print(classification_report(test,pred))

"""**Model 1: Logistic Regression with Unbalanced data**"""

X=data2.drop('target',axis=1)
Y=data2['target']
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.30)
clf=LogisticRegression()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
metrics(y_test,y_pred)

"""Although accuracy of the model is 96%, the F1-score is Zero. It is because the data is highly unbalanced with very few claimed data.

**Model 2: Logistic Regression with Unbalanced data + One-Hot encoded+ Scaled data**
"""

X=data3.drop('target',axis=1)
Y=data3['target']
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.30)
clf=LogisticRegression()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
metrics(y_test,y_pred)

"""As we can see here that even after encoding the categorical variables still there is no change in the result. The model is correctly classifying the Zeros and incorrectly classifying the Ones. Therefore,F1 score is still Zero.

**Model 3: Logistic Regression with Balanced data**
"""

X_train3_res.shape,Y_train3_res.shape

X_train,X_test,y_train,y_test=train_test_split(X_train3_res,Y_train3_res,test_size=0.30)
clf=LogisticRegression()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
metrics(y_test,y_pred)
Counter(y_test)

"""It can be seen that the data balancing has a positive impact on the result. The f1-score has increased from zero to 0.57.

**MODEL 3 WITH GRIDSEARCH CV**
"""

X_train,X_test,y_train,y_test=train_test_split(X_train_res,y_train_res,test_size=0.30)
clf=LogisticRegression()
grid={"C":np.logspace(-3,3,7), "penalty":["l1","l2"]}
clf=GridSearchCV(clf,grid,cv=10)
clf.fit(X_train,y_train)
print("tuned hpyerparameters :(best parameters) ",clf.best_params_)
print("accuracy :",clf.best_score_)

"""Still not much improvement in the accuracy. The logistic regression doesn't works well with datasets having large number of features.

**Model 4: Logistic Regression with Balanced data + One-Hot encoded+ Scaled data**
"""

X_train,X_test,y_train,y_test=train_test_split(X_train_res,y_train_res,test_size=0.30)
clf=LogisticRegression()
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
metrics(y_test,y_pred)
Counter(y_test)

"""There is not much change in the result after encoding the categorical variables and scaling the data. The accuracy of the model just increased from 58% to 59% whereas F1 score of One increased from 0.58 to 0.59.It maybe due to the curse of dimensionality. Therefore, instead of using all the features we need to select some best featutes.

**MODEL 4 WITH GRIDSEARCH CV**
"""

grid={"C":np.logspace(-3,3,7), "penalty":["l1","l2"]}
clf4_1=LogisticRegression()
clf4_cv=GridSearchCV(clf4_1,grid,cv=10)
clf4_cv.fit(X_train4,y_train4)
print("tuned hpyerparameters :(best parameters) ",clf4_cv.best_params_)
print("accuracy :",clf4_cv.best_score_)

"""**Linear_SVC UNBALANCED**"""

X=data2.drop('target',axis=1)
Y=data2['target']
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.30)
lsvc=LinearSVC(penalty='l1', dual=False,max_iter=2000)
lsvc.fit(X_train,y_train)
y_pred=lsvc.predict(X_test)
metrics(y_test,y_pred)
Counter(y_test)

"""**Linear_SVC with sample of BALANCED data**"""

temp=pd.concat([X_train3_res,Y_train3_res],axis=1)
temp1=temp.sample(300000,random_state=42)
X=temp1.drop('target',axis=1)
Y=temp1['target']
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.30)
lsvc=LinearSVC(penalty='l1', dual=False,max_iter=2000)
lsvc.fit(X_train,y_train)
y_pred=lsvc.predict(X_test)
metrics(y_test,y_pred)
Counter(y_test)

"""On testing this model on various sample size,it was found that after increasing the number of records beyond 300K, the f1-score is not improving further.

**Linear_SVC with important features**
"""

for i in np.arange(10,200,10):
  temp=pd.concat([X_train_res[imp_features[:i]],y_train_res],axis=1)
  temp1=temp.sample(220000,random_state=42)
  X=temp1.drop('target',axis=1)
  Y=temp1['target']
  X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.30)
  lsvc=LinearSVC(penalty='l1', dual=False,max_iter=5000)
  lsvc.fit(X_train,y_train)
  y_pred=lsvc.predict(X_test)
  print(i,'','F1_SCORE=',f1_score(y_test, y_pred))

"""### **XGBoost Classifier**"""

## Hyper Parameter Optimization

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
    
}

classifier=xgboost.XGBClassifier()
random_search=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring='roc_auc',n_jobs=-1,cv=5,verbose=3)
random_search.fit(X_train_res[imp_features[:20]],y_train_res)
random_search.best_estimator_

classifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
              colsample_bynode=1, colsample_bytree=0.4, gamma=0.4,
              learning_rate=0.2, max_delta_step=0, max_depth=15,
              min_child_weight=3, missing=None, n_estimators=100, n_jobs=1,
              nthread=None, objective='binary:logistic', random_state=0,
              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
              silent=None, subsample=1, verbosity=1)

score=cross_val_score(classifier,X_train_res[imp_features[:20]],y_train_res,cv=10)

score.mean()

"""**Checking the Model for first n important features**"""

for i in np.arange(10,200,10):
  temp=pd.concat([X_train_res[imp_features[:i]],y_train_res],axis=1)
  temp1=temp.sample(200000,random_state=42)
  X_temp=temp1.drop('target',axis=1)
  Y_temp=temp1['target']
  X_train,X_test,y_train,y_test=train_test_split(X_temp,Y_temp,test_size=0.30)
  classifier.fit(X_train,y_train)
  y_pred=classifier.predict(X_test)
  #print('ACCURACY=',accuracy_score(y_test,y_pred))
  #print('BALANCED ACCURACY=',balanced_accuracy_score(y_test, y_pred))
  print(i,'','F1_SCORE=',f1_score(y_test, y_pred),'  ACCURACY=',accuracy_score(y_test,y_pred))
  #print('CONFUSION MATRIX')
  #print(confusion_matrix(y_test, y_pred))
  #print(classification_report(y_test, y_pred))
  #Counter(y_test)

"""The best accuracy and f1-score are achieved with first 50 important features.

**Cross validation of the model for first 50 important features**
"""

score=cross_val_score(classifier,X_train_res[imp_features[:50]],y_train_res,cv=10)
score.mean()

"""The accuracy of the model is very high

**CONFUSION MATRIX**
"""

X_train,X_test,y_train,y_test=train_test_split(X_train_res[imp_features[:50]],y_train_res,test_size=0.30)
  classifier.fit(X_train,y_train)
  y_pred=classifier.predict(X_test)
  metrics(y_test,y_pred)

"""**XgBoost performed really well with dataset having large number of features. The accuracy and f1-score comes out to be 99%**

**Let's check the performance of the model with Original UNBALANCED dataset. The percentage of claimed data is about 3%**.
"""

data3.shape

X=data3[imp_features[:50]]
Y=data3['target']
y_pred=classifier.predict(X)
metrics(Y,y_pred)

"""**The model achieved very high accuracy even with unbalanced dataset. From the confusion matrix we can observe only 1 missclassification of claimed data.**

### **AdaBoostClassifier**
"""

from sklearn.ensemble import AdaBoostClassifier
ada=AdaBoostClassifier(n_estimators=100, random_state=0)

for i in np.arange(10,200,10):
  temp=pd.concat([X_train_res[imp_features[:i]],y_train_res],axis=1)
  #temp1=temp.sample(200000,random_state=42)
  X_temp=temp1.drop('target',axis=1)
  Y_temp=temp1['target']
  X_train,X_test,y_train,y_test=train_test_split(X_temp,Y_temp,test_size=0.30)
  ada.fit(X_train,y_train)
  y_pred=ada.predict(X_test)
  print(i,'','F1_SCORE=',f1_score(y_test, y_pred),'  ACCURACY=',accuracy_score(y_test,y_pred))

"""### **MLPClassifier**"""

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier()

parameter_space = {
    'hidden_layer_sizes': [(206,128,128), (206,128,128,128), (206,128,128,64),(206,128,128,128)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.001,0.01,0.05],
    'learning_rate': ['constant','adaptive'],
}

"""**MLP with RandomizedSearchCV**"""

mlpclf = RandomizedSearchCV(clf,parameter_space,cv=10,scoring='roc_auc',n_iter=5,verbose=3)

search=mlpclf.fit(X_train_res,y_train_res)
search.best_params_

"""**Applying best parameters**"""

mlpclf = MLPClassifier(hidden_layer_sizes=(206,128,128,128,2),random_state=1, max_iter=10,learning_rate='adaptive',alpha=0.0001,
                       activation='relu',solver='adam')
mlpclf.fit(X_train_res,y_train_res)
y_pred=mlpclf.predict(X_test)
print('ACCURACY=',accuracy_score(y_test,y_pred))
print('BALANCED ACCURACY=',balanced_accuracy_score(y_test, y_pred))
print(i,'','F1_SCORE=',f1_score(y_test, y_pred),'  ACCURACY=',accuracy_score(y_test,y_pred))
print('CONFUSION MATRIX')
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
Counter(y_test)

mlpclf = MLPClassifier(hidden_layer_sizes=(206,128,128,128),random_state=1, max_iter=15,learning_rate='adaptive',alpha=0.0001,
                       activation='relu',solver='adam')
mlpclf.fit(X_train_res,y_train_res)
y_pred=mlpclf.predict(X_test)
print('ACCURACY=',accuracy_score(y_test,y_pred))
print('BALANCED ACCURACY=',balanced_accuracy_score(y_test, y_pred))
print(i,'','F1_SCORE=',f1_score(y_test, y_pred),'  ACCURACY=',accuracy_score(y_test,y_pred))
print('CONFUSION MATRIX')
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
Counter(y_test)

"""**After applying best parameters using RandomizedSearchCV in Sklearn MLPClassifier, the f1 score comes out to be 97%.**

### **KERAS MLPClassifier**
"""

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import RMSprop
batch_size = 128
num_classes = 2
epochs = 3

X_train_res.shape

X_train,X_test,y_train,y_test=train_test_split(X_train_res,y_train_res,test_size=0.30)

input_shape=X_train.shape[1]

y_train1=keras.utils.to_categorical(y_train)
y_test1=keras.utils.to_categorical(y_test)

model = Sequential()
model.add(Dense(128, activation='relu', input_shape=(input_shape,)))
#model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
#model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(128, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])

history = model.fit(X_train, y_train1,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_test, y_test1))

score = model.evaluate(X_test, y_test1, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""### **MODELLING SUMMARY**
- **Finally after analyzing the various models, the XgBoost and Sklearn MLPClassifier model turned out to be best in classification with an accuracy and f1-score of 0.97-0.98.**
- **The logistic model didn't performed well with both balanced and imbalanced dataset even after scaling and encoding categorical variables.The f1-score reached upto 0.58. It seems that the logistics regression is inefficient with dataset having large number of categorical feature**.
- **Lastly, the Keras MLPClassifier need some hypertuning.**
"""